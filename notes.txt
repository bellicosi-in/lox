//variable declaration
Variable declaration parsing begins in varDeclaration() 
and relies on a couple of other functions. 
First, parseVariable() consumes the identifier token for the variable name,
adds its lexeme to the chunk’s constant table as a string, and
then returns the constant table index where it was added. 
Then, after varDeclaration() compiles the initializer, 
it calls defineVariable() to emit the bytecode for storing the variable’s value in the global variable hash table.

“Declaring” is when the variable is added to the scope, and “defining” is when it becomes available for use.


NOTES:
CHUNKS OF BYTECODE:
1. To use the cache effectively , the way we represent code in memory should be dense and ordered like its read.
2. writing in bytecode means that we are ensuring portability as well as fast. vm is a simulated chip written in software that interprets the 
bytecode one instruction at a time.
3. each chunk will carry with it a list of the values that appear as literals in the program. To keep things simpler, we'll put all constants in there, even simple integers.


A VIRTUAL MACHINE
1. This is the backend. it executes the instructions. you hand it a chunk of code and it  literally runs it.
2. VM detects runtime errors and compiler detects the static errors. 
3. each turn through the loop in run we execute a sinlge bytecode instruction.
4. the ip tracks the bytecode through the chunk thats passed to the vm. ip always points to the instruction about to be executed.
5. in the VM the code goes through Interpret to run() where the major function is happening.  we also disassemble the instructions.
6. this is the stack based vm. but there is another vm which is the register based vm. in this the local variables live on the stack.

SCANNING ON DEMAND:
1. clox has three phases : a scanner, a compiler and a virtual machine.
2. tokens flow from scanner to compiler and chunks of bytecode from compiler to vm. 
3.the compiler initializes the scanner. 
4.the pipeline to scan, compile and execute is driven by interpret(). 
5. define scanToken(). then use the scanner to makeToken.
6. to identify reserved keywords, we use trie. each string trie contains is represented as a path through the tree of character nodes, as in our traversal above. 
7. this is the phase of lexical analysis.
8. for more info on state machines : craftinginterpreters.com/state

COMPILING EXPRESSIONS
1. A compiler has roughly two jobs : it parses the user's source code to understand what it means(semantic analysis) and then it takes that knowledge and outputs low level instructions
that produce the same semantics.
2. in this we will build the parsing and code generation first and then stitch them together with the code in the middle that uses Pratt's technique to parse lox's particular grammar,
and output the right bytecode.
3. front half of the compiler - define the advance function that goes to the next valid token and handles errors.
4. after we parse and understand a piece of the user's program, the next step is to translate that to a series of btyecode instructions.
5.we are defining relevant functions to deal with each type of token that we get after being parsed.
6.expressions that start with a particular expression are called prefix expressions.
7.binary operators are infix expressions.

TYPES OF VALUES:
1. define a value struct that has support for different kinds of values. 

STRINGS
1. using stack to store the smaller, atomic values and heap for larger, variable sized values, leads to 2 level representation.
2. data that lives on the heap will maintain a similar state for the garbage collector. we will call it obj
3.Because ObjString is an Obj, it also needs the state all Objs share. It accomplishes that by having its first field be an Obj. 
C specifies that struct fields are arranged in memory in the order that they are declared. 
Also, when you nest structs, the inner struct’s fields are expanded right in place

4. Objstring -> string -> copyString -> ALLOCATE -> reallocate -> heapChars -> allocateString -> allocateObject
                            |_____________________________________________________|


HASH TABLES
1. Hash table - a set of keys and values.
2.A hash function takes some larger blob of data and “hashes” it to produce a fixed-size integer hash code whose value depends on all of
 the bits of the original data. it has three main goals :
 a. it must be deterministic
 b. it mmust be uniform
 c. it must be fast.
3. the ratio of load count to capacity is the load factor.
4. we can use the hash table to look up the variables and the values associated with it. 
5. we need a way to take string and convert it into a fixed size integer. 
6. each objstring stores the hash code for its string
7. string interning - create a collection of internal strings.  you only add unique strings to the collection.

GLOBAL VARIABLES
1. declarations are those statements that bind a new name to a value.
2. the other kind of statements - control flow, print etc. are just called statements.
3. each expression leaves one result value on stack. while the total net effect on the stack of a statement is 0.
4.  table globals is the hash table where the variables(global) gets stored.

LOCAL VARIABLES
1. for local variables, we dont need to stuff the variable's name into the constant table, so if the declaration is inside a local scope,
we return a dummy table index instead. 
2. there's no code to create a local variable at runtime
3. the compiler does need to remember that the variable exists for local variable.
4. “Declaring” is when the variable is added to the scope, and “defining” is when it becomes available for use.


JUMPING BACK AND FORTH
1.by flow we mena the way execution moves through the text of the program.
2. the VM's ip field stores the address of the current bytecode instruction. the value of that field is exactly "where we are " in the program.
3. to skip over a chunk of code, we simply set the ip field to the address of the bytecode instructions following that code. ``
4. for logical operators, we tend to fast execute if a single true or false expression to get the fastest result.
5. It has three clauses, all of which are optional:

The initializer can be a variable declaration or an expression. It runs once at the beginning of the statement.
The condition clause is an expression. Like in a while loop, we exit the loop when it evaluates to something falsey.
The increment expression runs once at the end of each loop iteration.

CALLS AND FUNCTIONS
1. we give each function its own chunk along with some metadata
2. Activation record - a region of storage set aside to hold control information and data storage associated with a single instance of a single procedure.
3. activation record pointer to locate the current AR, the compiler arranges to keep a pointer to AR, the activation record pointer in a designated register.
4. the current chunk is always the chunk owned by the function we're in the middle of compiling.
5. the compiler implicitly claims a stack slot zero for the vm's own internal use. we give it an empty name so that the user cant refer to it.
6. the recorded location where the function's locals start is a frame pointer.  
7. for each live function invocation- we need to track where on the stack that function's locals begin and where the call should resume.
8. instead of storing the return addresses in the callee's frame, the caller's stores its own ip. when we return from a function, the VM will jump to the ip of the
caller's callFrame and resume from here. 
9.each time a function is called, we create a callframe struct.
10. A programming language implementation reaches out and touches the material world through native functions. if you want to be able to write programs that check the
time, read user input or access the file system, we need to add native funcitons-callable from lox but implemented in C - that expose those capabilities.

CLOSURES
1. A closure is a record storing function together with an environment: a mapping associating each free variable of the function with the value or storage 
location to which the name was bound when the closure was created. A closure, unlike a plain function, allows the function to access those captured variables
through the closure's reference to them, even when the function is invoked outside their scope.
2. How Closures Work
Creating a Closure:
When you define a function inside another function, and the inner function accesses variables from the outer function, a closure is formed.
The inner function (the closure) maintains references to these outer variables even after the outer function has completed execution.
3. Note that the new call to addUpvalue() passes false for the isLocal parameter. Now you see that that flag controls whether the closure captures a local variable or an upvalue from the surrounding function.
4. each OP_CLOSURE instruction is followed by the series of bytes that specify the upvalues that the ObjClosure should own.
5. Closures capture variables. You can think of them as capturing the place the value lives. 
This is important to keep in mind as we deal with closed-over variables that are no longer on the stack.
When a variable moves to the heap, we need to ensure that all closures capturing that variable retain a reference to its one new location.
 That way, when the variable is mutated, all closures see the change.

GARBAGE COLLECTION
1. VMs consider a piece of memory to still be in use if it could possibly be read in the future.
2. A conservative gc is a special kind of collector that cinsiders any piece of memory to be a pointer if the value in there looks like it could be an address
3. A precise gc knows eactly which words in memory are pointers and which store other kinds of calues like numbers or strings.
4. A value is reachable if there is some way for a user program to reference it. 
5. A root is any object that the VM can reach directly without oging through a reference in some ohter object. Most roots are global variables or on the stack,
but as we'll see, there are a couple of other places the VM stores references to objects that it can find,
6. reachablility:
a. all roots are reachable.
b. any object referred t from a reachable object is itself reachable.
7. any values that doesnt meet the above definition is fair game.

8. the two fundamental steps for garbage collection is a recursive algorithm:
a. starting witht the roots, traverse through object refernces to fund the full set of reachable objects.
b. free all objects not in that set.

9.MARK- SWEEP :
    a. we start with the roots and traverse or trace through all of the objects those roots refer to. this is a classic graph traversal of all ther reachable
    objects. each time we visit an object, we mark it in some way.
    b. once the mark phase completes, every reachable object in the heao has been marked, that means any unmarked object is unreachable and ripe of reclamation.
    we go through all the unmarked objects and free each one.

10. A tracing garbage collector is any algorithm that races through the graph of object referencess. 
11. As the collector wanders through the graph of objects, we need to make sure it doesnt lose track of where it is o get stuch going in circles. fro this purpose
VM hackers came up with a metaphor called tricolor abstraction. Each object has a concpetual color that tracks what state the object is in, and what work is
left to do.
a. white = at the beginning of garbage collection, every object is white. that color means we have not reached or processed the object at all.
b. gray = during marking when we first reach an object, we darken it gray. this color means we know the obejct itself is reachable and should not be collected. but we have not yet
traced through it to see what other objects it references.  in graph algorithm terms, this is the worklist - the set of objects we know about but haven't processed it yet.
c. black = when we take a gray object and mark all of th eobjects it references, we then turn the gray object black, this color means the mark phase is done processing
that object.

12. pertaining to when the gc should run: two fundamental numbers used when measuring a memory manager's performance : throughput and latency
a. every managed language pays a performance price compared to explicit, user- authored deallocation. the time mostly is taken in figuring out which memory to free.
b. 1. Throughput - it is the total fraction of time spend running user code versus doing garbage collection work. 
    2. latency - it is the longest continuous chunk of time where the user's program is completely paused while garbage collection happens.


CLASSES AND INSTANCES

1. classses serve two main purposes in a language:
    a. they are how you create new instances
    b. they contain methods.
2. 

METHODS

1. for each method declaration, we emit a new OP-METHOD instruction that adds a
single method to that class.
2. while the user sees it as a single atomic operation, we implement it as a series
of mutations.
3. to define a new method, the VM needs three things:
    a. the name of the method.
    b. the closure for the method body.
    c. the class to bind the method to.
In the Context of Clox
In Clox (from "Crafting Interpreters"), a similar concept applies:

When a method is defined in a class, it doesn't inherently belong to any particular instance.
When you access a method on an instance of a class (e.g., myInstance.myMethod()), this within myMethod is automatically bound to myInstance.
This binding allows the method to interact with the specific instance (myInstance in this case), using this to refer to the instance.
Why It Matters
This concept is vital for:

Encapsulation: Methods can operate on their own object's data.
Reusability: The same method can be used across different instances of a class, with this allowing each method call to be specific to its instance.
Conclusion
The automatic binding of this to the instance a method is accessed from is a key feature of object-oriented programming. It provides methods with context 
about which object they are operating on, enabling more dynamic and reusable code structures.

The compiler keeps track of which slots in the function’s stack window are owned by which local variables. If you recall, the compiler sets aside stack slot zero by declaring a local variable whose name is an empty string.

For function calls, that slot ends up holding the function being called. Since the slot has no name, 
the function body never accesses it. You can guess where this is going. For method calls, we can repurpose that slot to store the receiver. 
Slot zero will store the instance that this is bound to. In order to compile this expressions, the compiler simply needs to give the correct name to that local variable.

Initializers work mostly like normal methods, with a few tweaks:
1. the runtime automatically invokes the initializer method whenever an instance
of a class is created.
2. the caller that constructs an instance always gets the instance back after
the initializer finishes, regardless of what the initializer function itself returns.
the intializer method doesn't need to explicitly return this.
3. in fact, an initializer is prohibited from returning any value at all since the value would 
never be seen anyway.

OP_INVOKE :
does the work of two BYTECODES - OP_GET_PROPERTY & OP_CALL
it takes two operands:
1. the index of the property name in the constant table.
2. the number of arguments passed to the method.


SUPERCLASSES:

1. we use the copy down method for inheritance, where we transfer all the methods of superclasses onto the subclass. 
2. we first create the class OP_CLASS to create the class after which we invoke OP_INHERIT to transfer all the methods of superclass to the subclass. after this
we use the OP_METHOD to invoke the methods of the subclass, so that if there are same methods, subclass's method overrides the superclass's methods.

OPTIMIZATION:

1. modulo operator is much slower than the other arithmetic operators.
2. you can calculate a number modulo any power of two by simply AND-ing it with that power of two minus one.
3. NaN boxing : within a single 64-bit double, you can store all of the different floating point numeric values, a pointer or any of a couple of other sentinel values.
Half the memory usage of our current Value struct, while retaining all of the fidelity.